\section{User Interface}\label{sec:ui}

A standardized graphical user interface (GUI) was developed to be robust and intuitively usable by anyone. First and foremost the GUI is responsible for relaying environmental sensor and navigation data. The GUI is standardized for any ground robot that is able to traverse through a 3 degree of freedom (DOF) work space (X,Y, Theta) through an under-actuated controller. The user is able to control the robot's movement with a single virtual joystick controlling forward motion and turning. A wrapper is written to translate the joystick commands to actual motor commands. To use this GUI for other ground robot platforms, all that would need to be developed is the wrapper code that translates the joystick into movement. The second joystick is dedicated to allowing the user to manipulate the camera in real time, allowing for control over a pan-tilt unit attached to the camera base which the user can point to view regions of interest. The camera feed and joystick bandwidth is automatically adjusted and delegated based on the bandwidth that is available at a particular instance between the robotic platform and the control unit. While the control unit can be anywhere in the world, thus allowing the robot user to operate it through the internet or cloud service, there is better overall performance as well as lower latency in video stream and sensor data when the control unit is within the same compound as the robot. In ideal conditions, the user would be able to see and respond to any obstacle that is close by within low response time deltas, however under most conditions this isn't possible nor reasonable to expect. Low level obstacle avoidance function aids with basic navigation and reduces the probability of a crash that may be unforeseen by the user. The risk of the robot crashing into an obstacle cannot be completely mitigated because of several factors, but a few are real world delays in the data stream, loss of user's focus, lapses in data due to packet loss, the camera view is facing away from the obstacle, and connection loss.


/%Another feature that has been built into the GUI is controlling based on way points instead of direct control over the robot. This allows for the user to click on the map to command the robot to navigate to the destination if possible. A 2D map is generated using SLAM from a LIDAR mounted on the robot--only differentials in the map are sent over the network. So while the initial connection may be slow, the delay during use is reasonalble. The 2D point that the user inputs is then fed into the navigation stack to attempt to reach the end goal. A third mode that the robot can be put into is pure autonomous navigation where the user can view what the robot is doing but won't have any control over the robot. The controller's multiple functionality allows for the user to pick the most appropriate mode of operation for a particular point in time, or to help the robot if it ever gets stuck. This is an important consideration when developing a GUI that is meant for long term industrial use, as all robots will eventually encounter something that they don't know how to deal with. 

As mentioned in the previous paragraph, the GUI allows the user to utilize 2 joysticks with 1 being for robot motion, and the other for camera pan-tilt motion; this 2 joystick scheme constitutes the first control mode. The GUI is designed to feature 2 more control modes. The control modes change the level of autonomy the robot exhibits during its mission. This, of course, requires for the different control modes to be hard-coded into the robot to ensure full functionality, both in terms of autonomous motion and safety of equipment in the facility being traversed. The change in control modes helps reduce the probability of mission failure or loss of robot if the control unit loses connection to the robot. The modes also change the level of involvement of the user in navigation duties, thus allowing the user to focus on higher level duties such as finding sources of contamination or thorough sterilization of the robot's local environment. This reduction of mental fatigue on the user helps increase missions success. All navigation modes utilize the GUI built-in features of map updating when differentials are found, obstacle proximity alerts, and environmental sensor warnings. 

The second navigation mode provided through the GUI lets the user move the robot via way point navigation, which is done through a 2-Dimensional (2-D) interactive map of the compound that is provided to both robot and control unit prior to start of the mission. The robot is given way point locations to reach on the interactive map by the user during the start of its mission to alleviate future bandwidth needs that may arise as the mission progresses, and as bandwidth becomes restricted with increased distance from the control unit. These way points are generated by the interactive map feature after translating user input given via a mouse click-action on the 2-D map. If, for any reason, the robot is incapable of navigating to a way point it sends a warning message to the user indicating that autonomy has been reduced, and the robot has reverted back to control mode 1. The user can then use 2 joysticks to manually  maneuver the robot further into the compound or return it to the start point. After a predetermined amount of time, with no user input in control mode 1, the robot elevates autonomy and reverses course to return to the mission's start position using the way points taken to arrive at its current position. 

The third navigation mode allows for full autonomous exploration of the environment regardless of having any apriori knowledge of the compound layout or not having a map. The robot uses LiDar-based SLAM-EKF to generate a map if none exists already. The map data is held in the on-board PC of the robot and can be transmitted on demand or streamed live. All navigation, and obstacle avoidance actions are recorded on the full 3-D map, as metadata, that is generated as the robot progresses through the compound. Video data can also be toggled on if the user desires to visually inspect mission progress. By default, in order to reduce bandwidth saturation, map data is not transmitted to let mission critical sensor data take transmission priority.  Even mode 3, with its full autonomy, has a fail-safe feature that sends a warning message to the user to indicate that the robot has reverted back to mode 1 for manual navigation. This only occurs if the robot is unable to continue through the compound or has encountered a navigational error in its on-board programming. As with mode 2, if the user takes no action or connection has been lost, the robot again elevates autonomy to its original setting to reverse course and return to the mission start position. 

The GUI allows for all the mentioned features to be toggled on or off through a settings window which is separate from the main robot interaction window in order to reduce the clutter the user sees while performing a mission. This will reduce the likelihood that the use will mistakenly click or press the wrong button during mission critical moments. The aforementioned navigation modes can be toggled on-the-fly by the user and are accessible directly from the main robot interaction window. The GUI is also based on Worcester Polytechnic Institute's (WPI's) Robot Web Tools (CHRIS PUT A CITATION HERE http://robotwebtools.org/)-  which is a set of modules that helps create web-based robot control apps, and lets Robot Operating System (ROS) software packages be accessible through a web interface that is constructed using HTML5 and Javascript. The GUI is essentially a web accessible overlay that allows the user to send and receive data across a connection between a ROS server, the control unit, and its corresponding ROS client, the robot. 
 
\begin{figure}
\centering
\includegraphics[width=3.4in]{pictures/Korpela_GUI.png}
\caption{System flow of individual robot interface.}
\label{gui_flow}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=3.4in]{pictures/Korpela_GUI.png}
\caption{Screenshot of the developed web based GUI allowing for view of the camera and 2D map.}
\label{gui_screenshot}
\end{figure}
