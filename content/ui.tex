\section{User Interface}\label{sec:ui}

A standardized graphical user interface (GUI) was developed to be robust and intuitively usable by anyone. First and foremost the GUI is responsible for relaying environmental sensor and navigation data. The GUI is standardized for any ground robot that is able to traverse through a 3 degree of freedom (DOF) work space (X,Y, Theta) through an under-actuated controller. The user is able to control the robot's movement with a single virtual joystick controlling forward motion and turning. A wrapper was written to translate the joystick to actual motor commands--all that would need to be developed for an individual robot is the wrapper that translates the joystick into movement. The second joystick is dedicated to allowing the user to manipulate the camera in real time--allowing for control over a pan-tilt configuration allows the user to point and view whatever is needed. The camera feed and joystick bandwidth is controlled based on the bandwidth that is currently possible between the robot and the control unit. While the control unit can be anywhere in the world--the controller is based in the web and based off of WPI's Robot Web Tools (CHRIS PUT A CITATION HERE http://robotwebtools.org/)-there is better performance when the control unit is within the same compound for less delay in image and other signals. In ideal conditions the user would be able to see and respond to any obstacle that is close by, however under most conditions this isn't possible. Low level obstacle avoidance helps the user to not cause a crash that is unable to be seen by the user, because of real world delay or if the camera was facing the wrong direction to see the obstacle.

Another feature that has been built into the GUI is controlling based on way points instead of direct control over the robot. This allows for the user to click on the map to command the robot to navigate to the destination if possible. A 2D map is generated using SLAM from a LIDAR mounted on the robot--only differentials in the map are sent over the network. So while the initial connection may be slow, the delay during use is reasonalble. The 2D point that the user inputs is then fed into the navigation stack to attempt to reach the end goal. A third mode that the robot can be put into is pure autonomous navigation where the user can view what the robot is doing but won't have any control over the robot. The controller's multiple functionality allows for the user to pick the most appropriate mode of operation for a particular point in time, or to help the robot if it ever gets stuck. This is an important consideration when developing a GUI that is meant for long term industrial use, as all robots will eventually encounter something that they don't know how to deal with. 