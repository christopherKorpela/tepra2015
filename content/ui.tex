\section{User Interface}\label{sec:ui}

A standardized graphical user interface (GUI) was developed to be robust and intuitively usable by anyone. First and foremost the GUI is responsible for relaying environmental sensor and navigation data. The GUI is standardized for any ground robot that is able to traverse through a 3 degree of freedom (DOF) work space (X,Y, Theta) through an under-actuated controller. The user is able to control the robot's movement with a single virtual joystick controlling forward motion and turning. A wrapper is written to translate the joystick commands to actual motor commands. To use this GUI for other ground robot platforms, all that would need to be developed is the wrapper code that translates the joystick into movement. The second joystick is dedicated to allowing the user to manipulate the camera in real time, allowing for control over a pan-tilt unit attached to the camera base which the user can use to point and view points of interest. The camera feed and joystick bandwidth is automatically adjusted and delegated based on the bandwidth that is currently  available between the robot and the control unit. While the control unit can be anywhere in the world, thus allowing the controller to operate through the internet cloud there is better performance when the control unit is within the same compound for less delay in image and other signals. In ideal conditions the user would be able to see and respond to any obstacle that is close by, however under most conditions this isn't possible. Low level obstacle avoidance helps the user to not cause a crash that is unable to be seen by the user, because of real world delay or if the camera was facing the wrong direction to see the obstacle.

and based off of WPI's Robot Web Tools (CHRIS PUT A CITATION HERE http://robotwebtools.org/)-  

Another feature that has been built into the GUI is controlling based on way points instead of direct control over the robot. This allows for the user to click on the map to command the robot to navigate to the destination if possible. A 2D map is generated using SLAM from a LIDAR mounted on the robot--only differentials in the map are sent over the network. So while the initial connection may be slow, the delay during use is reasonalble. The 2D point that the user inputs is then fed into the navigation stack to attempt to reach the end goal. A third mode that the robot can be put into is pure autonomous navigation where the user can view what the robot is doing but won't have any control over the robot. The controller's multiple functionality allows for the user to pick the most appropriate mode of operation for a particular point in time, or to help the robot if it ever gets stuck. This is an important consideration when developing a GUI that is meant for long term industrial use, as all robots will eventually encounter something that they don't know how to deal with. 

